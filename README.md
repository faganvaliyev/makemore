# ðŸ”¤ Makemore: From Bigrams to Mini-GPT
This repository is my personal learning journey inspired by [Andrej Karpathy's](https://www.youtube.com/@AndrejKarpathy) legendary series on building a character-level language model from scratch, step by step. The goal: **to understand how modern language models work under the hood** by implementing everything from basic bigrams to a tiny GPT.

## ðŸ“š Contents

| Section | Description |
|--------|-------------|
| `01_bigram/` | Bigram language model using raw counts and probabilities |
| `02_nn/` | Simple neural net for predicting the next character |
| `03_mlp/` | Feedforward MLP with embeddings |
| `04_batchnorm/` | BatchNorm and improved training tricks |
| `05_self_attention/` | Building self-attention from scratch |
| `06_transformer/` | Full transformer implementation |
| `07_gpt/` | Mini GPT-like model with training loop |
| `08_experiments/` | My own experiments, tweaks, and dataset extensions |

## ðŸ§  Why This Repo Exists

I created this repository to:
- Deeply understand each concept by implementing it myself
- Track my progress through Andrej's course
- Encourage others to do the same (learning by doing!)
- Explore extensions like new datasets, model improvements, and experiments
